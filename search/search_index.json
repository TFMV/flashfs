{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlashFS Documentation","text":"<p>FlashFS is a high-performance file system snapshot and diff tool designed for efficiently tracking and managing changes across file systems over time.</p>"},{"location":"#overview","title":"Overview","text":"<p>FlashFS allows you to:</p> <ul> <li>Create snapshots of your file system's state at a point in time</li> <li>Compare snapshots to identify exactly what has changed</li> <li>Generate and apply diffs between snapshots for efficient incremental backups</li> <li>Manage snapshot lifecycle with flexible expiry policies</li> <li>Query snapshots to find specific files or directories</li> </ul> <p>FlashFS is optimized for performance, with features like parallel processing, efficient binary serialization, content-based deduplication, and intelligent caching.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Streaming Directory Walker: Process files as they're discovered for improved memory efficiency and responsiveness</li> <li>Multiple Hashing Algorithms: Support for BLAKE3, MD5, SHA1, and SHA256</li> <li>Partial Hashing: Efficiently hash large files by sampling portions of the content</li> <li>Concurrent Processing: Utilize multiple CPU cores for faster operations</li> <li>Progress Reporting: Real-time feedback during long-running operations</li> <li>Memory Efficiency: Designed to handle very large directory structures with minimal memory footprint</li> <li>Flexible Configuration: Extensive options for customizing behavior</li> </ul>"},{"location":"#key-components","title":"Key Components","text":"<p>FlashFS consists of several modular components, each responsible for a specific aspect of the system:</p> Component Description CLI Command-line interface for interacting with FlashFS Walker Traverses file systems to collect metadata Serializer Converts metadata to efficient binary format Schema Defines data structures using FlatBuffers Storage Manages storage and retrieval of snapshots and diffs Diff Computation Computes and applies differences between snapshots Expiry Policy Manages snapshot lifecycle and cleanup Hash Provides efficient file hashing with multiple algorithms Buffer Pool Optimizes memory usage for I/O operations"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>go install github.com/TFMV/flashfs@latest\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Create a snapshot:</p> <pre><code>flashfs snapshot create --source /path/to/directory --output my-snapshot.snap\n</code></pre> <p>Compare two snapshots:</p> <pre><code>flashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff\n</code></pre> <p>Apply a diff to generate a new snapshot:</p> <pre><code>flashfs apply --base snapshot1.snap --diff changes.diff --output snapshot2.snap\n</code></pre> <p>List snapshots:</p> <pre><code>flashfs snapshot list --dir /path/to/snapshots\n</code></pre>"},{"location":"#advanced-features","title":"Advanced Features","text":"<p>FlashFS includes advanced features like:</p> <ul> <li>Bloom filters for rapid change detection</li> <li>Content-based deduplication using BLAKE3 hashing</li> <li>Parallel processing for faster snapshot and diff operations</li> <li>Configurable compression using zstd</li> <li>Efficient caching for frequently accessed snapshots</li> <li>Query capabilities for finding specific files in snapshots</li> <li>Partial hashing for efficient processing of large files</li> <li>Memory-efficient buffer pools for optimized I/O operations</li> <li>Streaming serialization for handling very large snapshots</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>FlashFS is designed for high performance:</p> <ul> <li>Fast snapshot creation with parallel file system traversal</li> <li>Efficient binary serialization using FlatBuffers</li> <li>Minimal memory usage with streaming processing</li> <li>Quick diff computation using Bloom filters for pre-filtering</li> <li>Optimal compression with zstd for storage efficiency</li> <li>High-performance hashing with BLAKE3 and buffer pooling</li> <li>Memory-efficient I/O with reusable buffer pools</li> <li>Chunked streaming for processing very large datasets</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Backup systems: Create incremental backups by storing only changes</li> <li>File synchronization: Identify differences between systems efficiently</li> <li>Change monitoring: Track file system changes over time</li> <li>Deployment verification: Ensure consistency across deployed systems</li> <li>Data migration: Track changes during migration processes</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed information on each component, please refer to the dedicated documentation pages linked in the Key Components section above.</p> <p>For information on advanced features and capabilities:</p> <ul> <li>Diff Computation: Learn how FlashFS efficiently computes and applies differences between snapshots</li> <li>Expiry Policy: Understand how to manage snapshot lifecycle and cleanup</li> <li>Streaming Processing: Discover how FlashFS handles very large datasets with minimal memory footprint</li> </ul>"},{"location":"USE_CASES_ROADMAP/","title":"FlashFS Use Cases and Roadmap","text":""},{"location":"USE_CASES_ROADMAP/#security-use-cases","title":"Security Use Cases","text":""},{"location":"USE_CASES_ROADMAP/#intrusion-detection","title":"Intrusion Detection","text":"<ul> <li>Current: Snapshot and diff capabilities to detect file changes.</li> <li>Planned: Real-time monitoring and alerting integration.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#forensic-analysis","title":"Forensic Analysis","text":"<ul> <li>Current: Ability to store and retrieve snapshots.</li> <li>Planned: Enhanced querying and filtering tools for forensic investigations.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#compliance-verification","title":"Compliance Verification","text":"<ul> <li>Current: Snapshot comparison for compliance checks.</li> <li>Planned: Automated compliance reporting and integration with compliance frameworks.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#devops-and-infrastructure","title":"DevOps and Infrastructure","text":""},{"location":"USE_CASES_ROADMAP/#configuration-management","title":"Configuration Management","text":"<ul> <li>Current: Snapshot and diff for configuration drift detection.</li> <li>Planned: Integration with popular configuration management tools (e.g., Ansible, Terraform).</li> </ul>"},{"location":"USE_CASES_ROADMAP/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>Current: Snapshot storage and retrieval.</li> <li>Planned: Automated disaster recovery workflows and cloud integration.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#infrastructure-as-code-validation","title":"Infrastructure as Code Validation","text":"<ul> <li>Current: Basic snapshot validation.</li> <li>Planned: Advanced validation rules and integration with CI/CD pipelines.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#development-and-testing","title":"Development and Testing","text":""},{"location":"USE_CASES_ROADMAP/#development-environment-management","title":"Development Environment Management","text":"<ul> <li>Current: Snapshot and restore capabilities.</li> <li>Planned: Enhanced environment cloning and management tools.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#testing-and-qa","title":"Testing and QA","text":"<ul> <li>Current: Snapshot comparison for regression testing.</li> <li>Planned: Integration with automated testing frameworks.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#release-management","title":"Release Management","text":"<ul> <li>Current: Snapshot tagging and retrieval.</li> <li>Planned: Automated release workflows and version management.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#data-management","title":"Data Management","text":""},{"location":"USE_CASES_ROADMAP/#data-migration","title":"Data Migration","text":"<ul> <li>Current: Snapshot and restore for data migration.</li> <li>Planned: Optimized migration tools and cloud storage integration.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#content-distribution","title":"Content Distribution","text":"<ul> <li>Current: Snapshot storage and retrieval.</li> <li>Planned: CDN integration and optimized distribution workflows.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#version-control-for-unmanaged-files","title":"Version Control for Unmanaged Files","text":"<ul> <li>Current: Basic snapshot versioning.</li> <li>Planned: Enhanced version control features and integration with existing VCS.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#specialized-use-cases","title":"Specialized Use Cases","text":""},{"location":"USE_CASES_ROADMAP/#iot-and-edge-computing","title":"IoT and Edge Computing","text":"<ul> <li>Current: Lightweight snapshot capabilities.</li> <li>Planned: Optimized edge deployment and management tools.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#machine-learning-operations","title":"Machine Learning Operations","text":"<ul> <li>Current: Snapshot storage for ML datasets.</li> <li>Planned: Integration with ML workflow management tools.</li> </ul>"},{"location":"USE_CASES_ROADMAP/#database-operations","title":"Database Operations","text":"<ul> <li>Current: Snapshot and restore for database backups.</li> <li>Planned: Enhanced database-specific snapshot tools and integrations.</li> </ul> <p>This roadmap outlines the current capabilities of FlashFS and planned improvements to enhance usability and integration across various domains.</p>"},{"location":"buffer-pool/","title":"Buffer Pool","text":"<p>The Buffer Pool module provides memory-efficient buffer management for FlashFS operations, particularly for file hashing and I/O operations. It reduces memory allocations and garbage collection pressure by reusing byte slices.</p>"},{"location":"buffer-pool/#overview","title":"Overview","text":"<p>The Buffer Pool is designed to:</p> <ol> <li>Provide a pool of reusable byte slices</li> <li>Reduce memory allocations and garbage collection overhead</li> <li>Optimize performance for I/O-intensive operations</li> <li>Track usage metrics for monitoring and tuning</li> <li>Support configurable buffer sizes and pool behavior</li> </ol>"},{"location":"buffer-pool/#key-components","title":"Key Components","text":""},{"location":"buffer-pool/#bufferpool","title":"BufferPool","text":"<p>The core component is the <code>BufferPool</code> struct, which manages a pool of byte slices:</p> <pre><code>type BufferPool struct {\n    pool                 sync.Pool\n    size                 int\n    name                 string\n    metrics              *BufferPoolMetrics\n    minSize              int\n    maxSize              int\n    shortageNotification chan struct{}\n    getTimeout           time.Duration\n}\n</code></pre>"},{"location":"buffer-pool/#bufferpooloptions","title":"BufferPoolOptions","text":"<p>The behavior of a buffer pool can be configured using the <code>BufferPoolOptions</code> struct:</p> <pre><code>type BufferPoolOptions struct {\n    BufferSize           int           // Initial and default buffer size\n    MinBufferSize        int           // Minimum buffer size to accept back into the pool\n    MaxBufferSize        int           // Maximum buffer size to create\n    PoolName             string        // Name for the pool (for metrics/logging)\n    ShortageNotification chan struct{} // Channel for communicating buffer supply shortages\n    GetTimeout           time.Duration // Timeout for waiting on a buffer\n}\n</code></pre>"},{"location":"buffer-pool/#bufferpoolmetrics","title":"BufferPoolMetrics","text":"<p>The <code>BufferPoolMetrics</code> struct tracks usage statistics for a buffer pool:</p> <pre><code>type BufferPoolMetrics struct {\n    Gets              uint64\n    Puts              uint64\n    NewAllocations    uint64\n    ResizeAllocations uint64\n    RejectedBuffers   uint64\n    Size              int\n    Timeouts          uint64\n    Name              string\n}\n</code></pre>"},{"location":"buffer-pool/#core-functions","title":"Core Functions","text":""},{"location":"buffer-pool/#creating-and-using-a-buffer-pool","title":"Creating and Using a Buffer Pool","text":"<pre><code>// Create a new buffer pool with custom options\noptions := hash.DefaultBufferPoolOptions()\noptions.BufferSize = 32 * 1024 * 1024 // 32MB buffers\noptions.PoolName = \"large-file-pool\"\npool := hash.NewBufferPool(options)\n\n// Get a buffer from the pool\nbuffer, err := pool.Get()\nif err != nil {\n    // Handle error\n}\n\n// Use the buffer for I/O operations\n// ...\n\n// Return the buffer to the pool when done\npool.Put(buffer)\n</code></pre>"},{"location":"buffer-pool/#using-the-default-buffer-pool","title":"Using the Default Buffer Pool","text":"<p>FlashFS provides a default buffer pool for convenience:</p> <pre><code>// Get a buffer from the default pool\nbuffer, err := hash.GetBuffer()\nif err != nil {\n    // Handle error\n}\n\n// Use the buffer\n// ...\n\n// Return the buffer to the default pool\nhash.PutBuffer(buffer)\n</code></pre>"},{"location":"buffer-pool/#buffer-management","title":"Buffer Management","text":"<p>The buffer pool implements several strategies to efficiently manage memory:</p> <ol> <li>Buffer Reuse: Buffers are returned to the pool after use and reused for subsequent operations</li> <li>Size Management: Buffers that are too small are rejected, while oversized buffers are trimmed</li> <li>Timeout Handling: Optional timeouts prevent blocking when buffers are scarce</li> <li>Shortage Notification: Optional channel notifications when buffer demand exceeds supply</li> </ol>"},{"location":"buffer-pool/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Buffer Size: Larger buffers improve throughput for sequential I/O but consume more memory</li> <li>Pool Size: The pool automatically grows and shrinks based on demand</li> <li>Concurrency: The buffer pool is thread-safe and can be used from multiple goroutines</li> <li>Memory Pressure: Monitor metrics to detect memory pressure and adjust buffer sizes accordingly</li> </ul>"},{"location":"buffer-pool/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>The buffer pool provides detailed metrics to help monitor and tune performance:</p> <pre><code>// Get metrics from a buffer pool\nmetrics := pool.Metrics()\nfmt.Println(metrics.String())\n\n// Output:\n// BufferPool 'large-file-pool' Metrics:\n//   Gets:               1250\n//   Puts:               1248\n//   New Allocations:    10\n//   Resize Allocations: 2\n//   Rejected Buffers:   5\n//   Size:               33554432\n//   Timeouts:           0\n</code></pre>"},{"location":"buffer-pool/#example-usage","title":"Example Usage","text":"<pre><code>// Create a custom buffer pool for large file operations\nlargeFilePool := hash.NewBufferPool(hash.BufferPoolOptions{\n    BufferSize:    64 * 1024 * 1024, // 64MB\n    MinBufferSize: 32 * 1024 * 1024, // 32MB\n    MaxBufferSize: 128 * 1024 * 1024, // 128MB\n    PoolName:      \"large-file-pool\",\n    GetTimeout:    5 * time.Second,\n})\n\n// Process multiple files using the pool\nfor _, filePath := range filePaths {\n    // Get a buffer from the pool\n    buffer, err := largeFilePool.Get()\n    if err != nil {\n        log.Printf(\"Failed to get buffer: %v\", err)\n        continue\n    }\n\n    // Use the buffer for file processing\n    file, err := os.Open(filePath)\n    if err != nil {\n        largeFilePool.Put(buffer) // Return buffer to pool\n        log.Printf(\"Failed to open file: %v\", err)\n        continue\n    }\n\n    // Process the file using the buffer\n    // ...\n\n    file.Close()\n\n    // Return the buffer to the pool\n    largeFilePool.Put(buffer)\n}\n\n// Print metrics\nfmt.Println(largeFilePool.Metrics().String())\n</code></pre>"},{"location":"buffer-pool/#integration-with-hash-module","title":"Integration with Hash Module","text":"<p>The Buffer Pool is tightly integrated with the Hash module to optimize memory usage during hashing operations:</p> <pre><code>// The hash.File function automatically uses the buffer pool\nresult := hash.File(path, hash.DefaultOptions())\n\n// The hash.Reader function also uses the buffer pool\nresult := hash.Reader(reader, hash.BLAKE3)\n</code></pre>"},{"location":"cli/","title":"Command Line Interface","text":"<p>FlashFS provides a comprehensive command-line interface (CLI) for interacting with snapshots, diffs, and expiry policies. The CLI is built using Cobra, a powerful library for creating modern CLI applications.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>The FlashFS CLI includes commands for:</p> <ul> <li>Creating and managing snapshots</li> <li>Computing and applying diffs between snapshots</li> <li>Querying snapshot contents</li> <li>Managing snapshot expiry policies</li> </ul>"},{"location":"cli/#command-structure","title":"Command Structure","text":"<pre><code>flashfs\n\u251c\u2500\u2500 snapshot - Create a snapshot of a directory\n\u251c\u2500\u2500 diff - Compute differences between snapshots\n\u251c\u2500\u2500 apply - Apply a diff to a snapshot\n\u251c\u2500\u2500 query - Query snapshot contents\n\u2514\u2500\u2500 expiry - Manage snapshot expiry policies\n    \u251c\u2500\u2500 set - Set expiry policy\n    \u251c\u2500\u2500 apply - Apply expiry policy\n    \u2514\u2500\u2500 show - Show current expiry policy\n</code></pre>"},{"location":"cli/#global-flags","title":"Global Flags","text":"<p>These flags apply to all commands:</p> <pre><code>--help, -h     Show help for a command\n--verbose, -v  Enable verbose output\n</code></pre>"},{"location":"cli/#snapshot-command","title":"Snapshot Command","text":"<p>Create a snapshot of a directory.</p> <pre><code>flashfs snapshot [flags]\n</code></pre>"},{"location":"cli/#flags","title":"Flags","text":"<pre><code>--path, -p string     Directory to snapshot (required)\n--output, -o string   Output snapshot file (required)\n--exclude string      Exclude pattern (e.g., \"*.tmp,*.log\")\n</code></pre>"},{"location":"cli/#implementation-details","title":"Implementation Details","text":"<p>The snapshot command uses the streaming walker implementation (<code>WalkStreamWithCallback</code>) to efficiently process large directory structures. This provides several benefits:</p> <ol> <li>Memory Efficiency: Files are processed as they're discovered, keeping memory usage relatively constant regardless of directory size.</li> <li>Progress Reporting: The command shows real-time progress updates as files are processed.</li> <li>Responsiveness: Users see immediate feedback rather than waiting for the entire walk to complete.</li> <li>Cancellation Support: The operation can be cleanly cancelled with Ctrl+C.</li> </ol> <p>This makes the snapshot command suitable for very large directory structures, as it avoids loading all file metadata into memory at once.</p>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Create a snapshot of the current directory\nflashfs snapshot --path . --output backup.snap\n\n# Create a snapshot excluding temporary files\nflashfs snapshot --path /home/user/documents --output docs.snap --exclude \"*.tmp,*.bak\"\n\n# Create a snapshot without computing file hashes (faster but less accurate)\nflashfs snapshot --path /var/www --output web.snap --no-hash\n</code></pre>"},{"location":"cli/#diff-command","title":"Diff Command","text":"<p>Compute differences between snapshots and store them in a structured format.</p> <pre><code>flashfs diff [flags]\n</code></pre>"},{"location":"cli/#flags_1","title":"Flags","text":"<pre><code>--base, -b string     Base snapshot file (required)\n--target, -t string   Target snapshot file (required)\n--output, -o string   Output diff file (required)\n--detailed            Perform detailed comparison including file content hashes\n--parallel int        Number of parallel workers for comparison (default: number of CPU cores)\n--no-hash             Skip hash comparison (faster but less accurate)\n--path-filter string  Only compare files matching the specified path pattern\n</code></pre>"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code># Compute differences between two snapshots\nflashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff\n\n# Detailed comparison with 8 parallel workers\nflashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff --detailed --parallel 8\n\n# Compare only files in a specific directory\nflashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff --path-filter \"/home/user/documents/*\"\n</code></pre>"},{"location":"cli/#diff-format","title":"Diff Format","text":"<p>The generated diff file contains a structured representation of changes:</p> <ul> <li>Added files: Files that exist in the target snapshot but not in the base snapshot</li> <li>Modified files: Files that exist in both snapshots but have different metadata or content</li> <li>Deleted files: Files that exist in the base snapshot but not in the target snapshot</li> </ul> <p>Each change is stored as a <code>DiffEntry</code> with:</p> <ul> <li>Path of the changed file</li> <li>Type of change (added, modified, deleted)</li> <li>Before and after values for size, modification time, permissions, and content hash (as applicable)</li> </ul> <p>This structured format enables efficient application of changes and provides detailed information about what has changed between snapshots.</p>"},{"location":"cli/#apply-command","title":"Apply Command","text":"<p>Apply a diff to a snapshot to generate a new snapshot.</p> <pre><code>flashfs apply [flags]\n</code></pre>"},{"location":"cli/#flags_2","title":"Flags","text":"<pre><code>--base, -b string    Base snapshot file (required)\n--diff, -d string    Diff file to apply (required)\n--output, -o string  Output snapshot file (required)\n</code></pre>"},{"location":"cli/#examples_2","title":"Examples","text":"<pre><code># Apply a diff to generate a new snapshot\nflashfs apply --base snapshot1.snap --diff changes.diff --output snapshot2.snap\n</code></pre>"},{"location":"cli/#how-apply-works","title":"How Apply Works","text":"<p>The apply command:</p> <ol> <li>Loads the base snapshot into memory</li> <li>Deserializes the diff file into a structured Diff object</li> <li>Processes each DiffEntry based on its type:</li> <li>For added files (type 0): Creates a new entry in the snapshot</li> <li>For modified files (type 1): Updates the existing entry with new metadata</li> <li>For deleted files (type 2): Removes the entry from the snapshot</li> <li>Serializes the modified snapshot and writes it to the output file</li> </ol> <p>This structured approach ensures that changes are applied correctly and efficiently, maintaining the integrity of your file system representation.</p>"},{"location":"cli/#query-command","title":"Query Command","text":"<p>Query snapshot contents.</p> <pre><code>flashfs query [flags]\n</code></pre>"},{"location":"cli/#flags_3","title":"Flags","text":"<pre><code>--snapshot, -s string       Snapshot file to query (required)\n--path string               Path pattern to filter by (e.g., \"/home/user/*\")\n--modified-after string     Show files modified after this date (format: YYYY-MM-DD)\n--modified-before string    Show files modified before this date (format: YYYY-MM-DD)\n--size-gt int               Show files larger than this size (in bytes)\n--size-lt int               Show files smaller than this size (in bytes)\n--is-dir                    Show only directories\n--is-file                   Show only files\n--format string             Output format: text, json, csv (default \"text\")\n</code></pre>"},{"location":"cli/#examples_3","title":"Examples","text":"<pre><code># List all files in a snapshot\nflashfs query --snapshot backup.snap\n\n# Find large files modified recently\nflashfs query --snapshot backup.snap --size-gt 10485760 --modified-after \"2023-01-01\"\n\n# Find all .log files in a specific directory\nflashfs query --snapshot backup.snap --path \"/var/log/*.log\"\n\n# Export results as JSON\nflashfs query --snapshot backup.snap --path \"*.mp4\" --format json &gt; videos.json\n</code></pre>"},{"location":"cli/#expiry-command","title":"Expiry Command","text":"<p>Manage snapshot expiry policies.</p> <pre><code>flashfs expiry [command]\n</code></pre>"},{"location":"cli/#subcommands","title":"Subcommands","text":""},{"location":"cli/#set","title":"Set","text":"<p>Set the expiry policy for snapshots.</p> <pre><code>flashfs expiry set [flags]\n</code></pre>"},{"location":"cli/#flags_4","title":"Flags","text":"<pre><code>--max-snapshots int    Maximum number of snapshots to keep (0 = unlimited)\n--max-age string       Maximum age of snapshots to keep (e.g., 30d, 2w, 6m, 1y)\n--keep-hourly int      Number of hourly snapshots to keep\n--keep-daily int       Number of daily snapshots to keep\n--keep-weekly int      Number of weekly snapshots to keep\n--keep-monthly int     Number of monthly snapshots to keep\n--keep-yearly int      Number of yearly snapshots to keep\n--apply                Apply the policy immediately after setting it\n--dir string           Base directory for snapshots (default: current directory)\n</code></pre>"},{"location":"cli/#examples_4","title":"Examples","text":"<pre><code># Keep only the 10 most recent snapshots\nflashfs expiry set --max-snapshots 10\n\n# Remove snapshots older than 30 days\nflashfs expiry set --max-age 30d\n\n# Set a comprehensive retention policy\nflashfs expiry set --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 12 --keep-yearly 5\n\n# Set a policy and apply it immediately\nflashfs expiry set --max-age 30d --apply\n</code></pre>"},{"location":"cli/#apply","title":"Apply","text":"<p>Apply the current expiry policy to snapshots.</p> <pre><code>flashfs expiry apply [flags]\n</code></pre>"},{"location":"cli/#flags_5","title":"Flags","text":"<pre><code>--dir string  Base directory for snapshots (default: current directory)\n</code></pre>"},{"location":"cli/#examples_5","title":"Examples","text":"<pre><code># Apply the current expiry policy\nflashfs expiry apply\n\n# Apply the policy to snapshots in a specific directory\nflashfs expiry apply --dir /path/to/snapshots\n</code></pre>"},{"location":"cli/#show","title":"Show","text":"<p>Show the current expiry policy.</p> <pre><code>flashfs expiry show [flags]\n</code></pre>"},{"location":"cli/#flags_6","title":"Flags","text":"<pre><code>--dir string  Base directory for snapshots (default: current directory)\n</code></pre>"},{"location":"cli/#examples_6","title":"Examples","text":"<pre><code># Show the current expiry policy\nflashfs expiry show\n\n# Show the policy for snapshots in a specific directory\nflashfs expiry show --dir /path/to/snapshots\n</code></pre>"},{"location":"cli/#implementation-details_1","title":"Implementation Details","text":""},{"location":"cli/#command-registration","title":"Command Registration","text":"<p>Commands are registered in the <code>cmd</code> package:</p> <pre><code>func init() {\n    RootCmd.AddCommand(snapshotCmd)\n    RootCmd.AddCommand(diffCmd)\n    RootCmd.AddCommand(applyCmd)\n    RootCmd.AddCommand(queryCmd)\n    RootCmd.AddCommand(expiryCmd)\n\n    // Register expiry subcommands\n    expiryCmd.AddCommand(setExpiryCmd)\n    expiryCmd.AddCommand(applyExpiryCmd)\n    expiryCmd.AddCommand(showExpiryCmd)\n}\n</code></pre>"},{"location":"cli/#command-execution","title":"Command Execution","text":"<p>Each command is implemented as a Cobra command with a <code>RunE</code> function:</p> <pre><code>var snapshotCmd = &amp;cobra.Command{\n    Use:   \"snapshot\",\n    Short: \"Create a snapshot of a directory\",\n    Long:  `Create a snapshot of a directory, capturing file metadata and optionally content hashes.`,\n    RunE: func(cmd *cobra.Command, args []string) error {\n        // Command implementation\n        // ...\n        return nil\n    },\n}\n</code></pre>"},{"location":"cli/#error-handling","title":"Error Handling","text":"<p>The CLI provides detailed error messages and appropriate exit codes:</p> <pre><code>if err != nil {\n    fmt.Fprintf(os.Stderr, \"Error: %v\\n\", err)\n    os.Exit(1)\n}\n</code></pre>"},{"location":"cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"cli/#scripting","title":"Scripting","text":"<p>FlashFS commands can be used in scripts for automation:</p> <pre><code>#!/bin/bash\n# Create daily snapshots and apply expiry policy\n\n# Create a snapshot with the current date\nDATE=$(date +%Y%m%d)\nflashfs snapshot --path /home/user/documents --output backup-$DATE.snap\n\n# Apply expiry policy to clean up old snapshots\nflashfs expiry apply\n</code></pre>"},{"location":"cli/#piping-output","title":"Piping Output","text":"<p>Query results can be piped to other commands:</p> <pre><code># Find large files and sort by size\nflashfs query --snapshot backup.snap --size-gt 10485760 --format csv | sort -t, -k2 -n\n\n# Find recent changes and send a report by email\nflashfs query --snapshot backup.snap --modified-after \"2023-01-01\" | mail -s \"Recent Changes\" user@example.com\n</code></pre>"},{"location":"cli/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>FlashFS can be integrated with other tools:</p> <pre><code># Use with find to process multiple directories\nfind /home -type d -name \"projects\" | xargs -I{} flashfs snapshot --path {} --output {}.snap\n\n# Use with cron for scheduled snapshots\n# Add to crontab: 0 0 * * * /path/to/snapshot_script.sh\n</code></pre>"},{"location":"cloud-storage/","title":"Cloud Storage Integration","text":"<p>FlashFS provides integration with cloud storage services, allowing you to export snapshots to and restore them from cloud storage providers like Amazon S3, Google Cloud Storage (GCS), or any S3-compatible storage service (like MinIO).</p>"},{"location":"cloud-storage/#overview","title":"Overview","text":"<p>The cloud storage integration enables:</p> <ul> <li>Exporting snapshots to cloud storage for backup or archival purposes</li> <li>Restoring snapshots from cloud storage to a local directory</li> <li>Compression and decompression of snapshots during transfer</li> <li>Support for multiple cloud storage providers</li> </ul>"},{"location":"cloud-storage/#supported-storage-providers","title":"Supported Storage Providers","text":"<p>FlashFS currently supports the following cloud storage providers:</p> <ul> <li>Amazon S3: Native support for Amazon S3 storage</li> <li>Google Cloud Storage (GCS): Native support for Google Cloud Storage</li> <li>S3-Compatible Storage: Support for MinIO, Ceph, and other S3-compatible storage services</li> </ul>"},{"location":"cloud-storage/#configuration","title":"Configuration","text":"<p>Cloud storage configuration is primarily done through environment variables:</p>"},{"location":"cloud-storage/#amazon-s3-configuration","title":"Amazon S3 Configuration","text":"<pre><code># Required\nexport S3_ACCESS_KEY=your-access-key\nexport S3_SECRET_KEY=your-secret-key\n\n# Optional\nexport S3_ENDPOINT=https://s3.amazonaws.com  # Custom endpoint for S3-compatible storage\nexport S3_REGION=us-east-1                   # AWS region (default: us-east-1)\nexport S3_INSECURE=false                     # Use insecure connection (default: false)\nexport S3_FORCE_PATH_STYLE=false             # Use path-style addressing (default: false)\n</code></pre>"},{"location":"cloud-storage/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<pre><code># Required for service account authentication\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"cloud-storage/#usage","title":"Usage","text":""},{"location":"cloud-storage/#exporting-snapshots","title":"Exporting Snapshots","text":"<p>The <code>export</code> command allows you to export snapshots to cloud storage:</p> <pre><code>flashfs export [destination] [flags]\n</code></pre> <p>Where <code>destination</code> is a URL in the format <code>s3://bucket-name/prefix/</code> or <code>gcs://bucket-name/prefix/</code>.</p>"},{"location":"cloud-storage/#examples","title":"Examples","text":"<p>Export a specific snapshot to S3:</p> <pre><code>flashfs export s3://mybucket/flashfs-backups/ --snapshot snapshot1.snap\n</code></pre> <p>Export all snapshots to GCS:</p> <pre><code>flashfs export gcs://my-bucket-name/ --all\n</code></pre> <p>Export to MinIO or other S3-compatible storage:</p> <pre><code>export S3_ENDPOINT=https://minio.example.com\nexport S3_ACCESS_KEY=your-access-key\nexport S3_SECRET_KEY=your-secret-key\nexport S3_FORCE_PATH_STYLE=true\nflashfs export s3://mybucket/backups/ --all\n</code></pre>"},{"location":"cloud-storage/#flags","title":"Flags","text":"<ul> <li><code>--dir string</code>: Directory containing snapshots (default \"snapshots\")</li> <li><code>--snapshot string</code>: Name of the snapshot to export</li> <li><code>--all</code>: Export all snapshots</li> <li><code>--compress</code>: Compress snapshots before uploading (default true)</li> </ul>"},{"location":"cloud-storage/#restoring-snapshots","title":"Restoring Snapshots","text":"<p>The <code>restore</code> command allows you to restore snapshots from cloud storage:</p> <pre><code>flashfs restore [source] [flags]\n</code></pre> <p>Where <code>source</code> is a URL in the format <code>s3://bucket-name/prefix/</code> or <code>gcs://bucket-name/prefix/</code>.</p>"},{"location":"cloud-storage/#examples_1","title":"Examples","text":"<p>Restore a specific snapshot from S3:</p> <pre><code>flashfs restore s3://mybucket/flashfs-backups/ --snapshot snapshot1.snap\n</code></pre> <p>Restore all snapshots from GCS:</p> <pre><code>flashfs restore gcs://my-bucket-name/ --all\n</code></pre>"},{"location":"cloud-storage/#flags_1","title":"Flags","text":"<ul> <li><code>--dir string</code>: Directory to restore snapshots to (default \"snapshots\")</li> <li><code>--snapshot string</code>: Name of the snapshot to restore</li> <li><code>--all</code>: Restore all snapshots</li> <li><code>--decompress</code>: Decompress snapshots after downloading (default true)</li> </ul>"},{"location":"cloud-storage/#implementation-details","title":"Implementation Details","text":"<p>The cloud storage integration is implemented using the Thanos Object Storage library, which provides a unified interface for interacting with different cloud storage providers.</p>"},{"location":"cloud-storage/#compression","title":"Compression","text":"<p>Snapshots are compressed using the Zstandard compression algorithm before being uploaded to cloud storage. This reduces the amount of data transferred and the storage space required.</p>"},{"location":"cloud-storage/#error-handling","title":"Error Handling","text":"<p>The cloud storage integration includes robust error handling for common issues:</p> <ul> <li>Authentication errors (missing or invalid credentials)</li> <li>Network connectivity issues</li> <li>Permission issues</li> <li>Storage quota issues</li> </ul>"},{"location":"cloud-storage/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Streaming Uploads/Downloads: Files are streamed to/from cloud storage to minimize memory usage</li> <li>Compression: Zstandard compression provides a good balance between compression ratio and speed</li> <li>Parallel Processing: Multiple snapshots can be exported/restored in parallel</li> </ul>"},{"location":"cloud-storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cloud-storage/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors:</li> <li>Ensure that the required environment variables are set correctly</li> <li> <p>Check that the credentials have the necessary permissions</p> </li> <li> <p>Network Issues:</p> </li> <li>Check your network connectivity</li> <li> <p>Verify that the endpoint URL is correct</p> </li> <li> <p>Permission Issues:</p> </li> <li> <p>Ensure that the credentials have the necessary permissions to read/write to the bucket</p> </li> <li> <p>Storage Quota Issues:</p> </li> <li>Check that you have sufficient storage quota in your cloud storage account</li> </ol>"},{"location":"cloud-storage/#logging","title":"Logging","text":"<p>The cloud storage integration includes detailed logging to help diagnose issues. You can view the logs by running the commands with the <code>--verbose</code> flag.</p>"},{"location":"diff/","title":"Diff Computation","text":"<p>FlashFS provides powerful diff computation capabilities that enable efficient comparison, storage, and application of changes between snapshots. This feature is essential for incremental backups, file synchronization, and monitoring changes over time.</p>"},{"location":"diff/#benefits","title":"Benefits","text":"<ul> <li>Storage Efficiency: Store only the changes between snapshots instead of full copies</li> <li>Transfer Optimization: Transmit only the differences when synchronizing snapshots</li> <li>Change Tracking: Easily identify what files have been added, modified, or deleted</li> <li>Performance: Utilize parallel processing and pre-filtering for fast comparisons</li> <li>Flexibility: Configure comparison options based on specific needs</li> </ul>"},{"location":"diff/#how-diff-computation-works","title":"How Diff Computation Works","text":"<p>FlashFS implements a multi-stage approach to efficiently compute differences between snapshots:</p>"},{"location":"diff/#1-bloom-filter-pre-check","title":"1. Bloom Filter Pre-check","text":"<p>Before performing detailed comparisons, FlashFS uses Bloom filters to quickly identify files that may have changed:</p> <ol> <li>Creates a Bloom filter from the base snapshot</li> <li>Tests each file in the target snapshot against the filter</li> <li>Files that fail the Bloom filter test are candidates for detailed comparison</li> </ol> <p>This pre-filtering step significantly reduces the number of files that need detailed comparison, especially in large snapshots where most files remain unchanged.</p>"},{"location":"diff/#2-detailed-comparison","title":"2. Detailed Comparison","text":"<p>For files identified by the Bloom filter, FlashFS performs a detailed comparison:</p> <ol> <li>Compares file metadata (path, size, modification time, permissions)</li> <li>Optionally compares file content hashes for detecting changes even when metadata is unchanged</li> <li>Identifies files that have been added, modified, or deleted</li> </ol>"},{"location":"diff/#3-structured-diff-representation","title":"3. Structured Diff Representation","text":"<p>FlashFS uses a structured schema to represent diffs:</p> <ol> <li>Each changed file is represented as a <code>DiffEntry</code> with:</li> <li>Path of the file</li> <li>Type of change (added, modified, deleted)</li> <li> <p>Before and after values for size, modification time, permissions, and content hash</p> </li> <li> <p>All entries are collected in a <code>Diff</code> object that provides:</p> </li> <li>Efficient serialization and deserialization</li> <li>Type-safe access to change information</li> <li>Structured representation of all changes</li> </ol>"},{"location":"diff/#4-parallel-processing","title":"4. Parallel Processing","text":"<p>To accelerate diff computation for large snapshots, FlashFS can distribute the comparison work across multiple CPU cores:</p> <ol> <li>Divides the file list into chunks</li> <li>Processes each chunk in parallel</li> <li>Combines the results into a unified diff</li> </ol>"},{"location":"diff/#5-diff-storage","title":"5. Diff Storage","text":"<p>The computed diff is stored in a compact format:</p> <ol> <li>Records only the changes (added, modified, deleted files)</li> <li>Uses the same efficient FlatBuffers serialization as snapshots</li> <li>Applies Zstd compression to minimize storage requirements</li> </ol>"},{"location":"diff/#command-reference","title":"Command Reference","text":""},{"location":"diff/#computing-a-diff","title":"Computing a Diff","text":"<pre><code>flashfs diff [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--base &lt;file&gt;</code>: Base snapshot file (required)</li> <li><code>--target &lt;file&gt;</code>: Target snapshot file (required)</li> <li><code>--output &lt;file&gt;</code>: Output diff file (required)</li> <li><code>--detailed</code>: Perform detailed comparison including file content hashes</li> <li><code>--parallel &lt;n&gt;</code>: Number of parallel workers for comparison (default: number of CPU cores)</li> <li><code>--no-hash</code>: Skip hash comparison (faster but less accurate)</li> <li><code>--path-filter &lt;pattern&gt;</code>: Only compare files matching the specified path pattern</li> </ul>"},{"location":"diff/#applying-a-diff","title":"Applying a Diff","text":"<pre><code>flashfs apply [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--base &lt;file&gt;</code>: Base snapshot file (required)</li> <li><code>--diff &lt;file&gt;</code>: Diff file to apply (required)</li> <li><code>--output &lt;file&gt;</code>: Output snapshot file (required)</li> </ul>"},{"location":"diff/#viewing-diff-information","title":"Viewing Diff Information","text":"<pre><code>flashfs diff-info [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--diff &lt;file&gt;</code>: Diff file to analyze (required)</li> <li><code>--verbose</code>: Show detailed information about each changed file</li> </ul>"},{"location":"diff/#examples","title":"Examples","text":""},{"location":"diff/#basic-diff-computation","title":"Basic Diff Computation","text":"<p>Compute the differences between two snapshots:</p> <pre><code>flashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff\n</code></pre>"},{"location":"diff/#detailed-comparison-with-parallel-processing","title":"Detailed Comparison with Parallel Processing","text":"<p>Perform a detailed comparison using 8 parallel workers:</p> <pre><code>flashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff --detailed --parallel 8\n</code></pre>"},{"location":"diff/#path-filtered-comparison","title":"Path-Filtered Comparison","text":"<p>Compare only files in a specific directory:</p> <pre><code>flashfs diff --base snapshot1.snap --target snapshot2.snap --output changes.diff --path-filter \"/home/user/documents/*\"\n</code></pre>"},{"location":"diff/#applying-a-diff-to-generate-a-new-snapshot","title":"Applying a Diff to Generate a New Snapshot","text":"<p>Apply a diff to a base snapshot to generate a new snapshot:</p> <pre><code>flashfs apply --base snapshot1.snap --diff changes.diff --output snapshot2.snap\n</code></pre>"},{"location":"diff/#viewing-diff-information_1","title":"Viewing Diff Information","text":"<p>Analyze the contents of a diff file:</p> <pre><code>flashfs diff-info --diff changes.diff --verbose\n</code></pre>"},{"location":"diff/#implementation-details","title":"Implementation Details","text":"<p>The diff computation is implemented in the <code>SnapshotStore</code> struct with the following key methods:</p> <ul> <li><code>ComputeDiff</code>: Computes the differences between two snapshots and returns a structured <code>Diff</code> object</li> <li><code>StoreDiff</code>: Stores the computed diff to a file</li> <li><code>ApplyDiff</code>: Applies a diff to a base snapshot to generate a new snapshot</li> </ul>"},{"location":"diff/#diff-schema","title":"Diff Schema","text":"<p>FlashFS uses a structured schema for representing diffs:</p> <pre><code>table DiffEntry {\n  path: string;\n  type: byte;  // 0 = added, 1 = modified, 2 = deleted\n  oldSize: long;\n  newSize: long;\n  oldMtime: long;\n  newMtime: long;\n  oldPermissions: uint;\n  newPermissions: uint;\n  oldHash: [ubyte];\n  newHash: [ubyte];\n}\n\ntable Diff {\n  entries: [DiffEntry];\n}\n</code></pre> <p>This schema provides a clear, structured representation of changes, with each <code>DiffEntry</code> capturing all relevant information about a change:</p> <ul> <li>For added files (<code>type = 0</code>), only the new metadata is relevant</li> <li>For modified files (<code>type = 1</code>), both old and new metadata are stored</li> <li>For deleted files (<code>type = 2</code>), only the old metadata is relevant</li> </ul>"},{"location":"diff/#computediff-implementation","title":"ComputeDiff Implementation","text":"<p>The <code>ComputeDiff</code> function compares two snapshots and produces a structured diff:</p> <ol> <li>Deserializes both snapshots</li> <li>Creates maps of file entries for efficient lookup</li> <li>Identifies added, modified, and deleted files</li> <li>Creates <code>DiffEntry</code> objects for each change</li> <li>Assembles all entries into a <code>Diff</code> object</li> <li>Serializes the diff using FlatBuffers</li> <li>Compresses the serialized data</li> </ol>"},{"location":"diff/#applydiff-implementation","title":"ApplyDiff Implementation","text":"<p>The <code>ApplyDiff</code> function applies a diff to a base snapshot:</p> <ol> <li>Deserializes the base snapshot</li> <li>Deserializes the diff</li> <li>Creates a map of base entries</li> <li>Processes each diff entry:</li> <li>For added files, creates a new entry</li> <li>For modified files, updates the existing entry</li> <li>For deleted files, removes the entry</li> <li>Builds a new snapshot with the modified entries</li> <li>Serializes and compresses the new snapshot</li> </ol> <p>The diff system uses the same efficient serialization and compression techniques as the snapshot system, ensuring consistent performance and storage efficiency.</p>"},{"location":"diff/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Bloom Filters: The Bloom filter pre-check significantly reduces comparison time for large snapshots</li> <li>Parallel Processing: Utilizing multiple CPU cores can dramatically speed up diff computation</li> <li>Hash Comparison: Enabling hash comparison provides more accurate results but increases computation time</li> <li>Path Filtering: Using path filters can focus the comparison on relevant files, reducing processing time</li> <li>Structured Diffs: The structured diff format allows for efficient processing and application of changes</li> </ul> <p>For optimal performance, adjust the parallelism level based on your system's capabilities and use path filters when only specific directories are of interest.</p>"},{"location":"expiry-policy/","title":"Snapshot Expiry Policy","text":"<p>FlashFS provides a robust snapshot lifecycle management system through configurable expiry policies. This feature allows you to automatically manage the retention and deletion of snapshots based on various criteria.</p>"},{"location":"expiry-policy/#benefits","title":"Benefits","text":"<ul> <li>Automated Cleanup: Automatically remove old or unnecessary snapshots</li> <li>Storage Optimization: Prevent storage space from being consumed by outdated snapshots</li> <li>Flexible Retention: Configure granular retention policies based on time periods</li> <li>Policy Combinations: Combine different policy types for comprehensive lifecycle management</li> </ul>"},{"location":"expiry-policy/#expiry-policy-types","title":"Expiry Policy Types","text":"<p>FlashFS supports several types of expiry policies that can be used individually or in combination:</p>"},{"location":"expiry-policy/#1-maximum-snapshots-limit","title":"1. Maximum Snapshots Limit","text":"<p>Limit the total number of snapshots to keep, removing the oldest ones when the limit is exceeded.</p> <pre><code>flashfs expiry set --max-snapshots 50\n</code></pre>"},{"location":"expiry-policy/#2-maximum-age-limit","title":"2. Maximum Age Limit","text":"<p>Automatically remove snapshots older than a specified duration.</p> <pre><code>flashfs expiry set --max-age 90d  # Keep snapshots for 90 days\n</code></pre> <p>Supported duration units:</p> <ul> <li><code>h</code> or <code>hour(s)</code> - Hours</li> <li><code>d</code> or <code>day(s)</code> - Days</li> <li><code>w</code> or <code>week(s)</code> - Weeks</li> <li><code>m</code> or <code>month(s)</code> - Months (30 days)</li> <li><code>y</code> or <code>year(s)</code> - Years (365 days)</li> </ul>"},{"location":"expiry-policy/#3-time-based-retention-policies","title":"3. Time-Based Retention Policies","text":"<p>Keep a specific number of snapshots at different time intervals:</p> <pre><code># Keep 24 hourly, 7 daily, 4 weekly, 12 monthly, and 5 yearly snapshots\nflashfs expiry set --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 12 --keep-yearly 5\n</code></pre> <ul> <li>Hourly: Keep the most recent snapshot from each hour</li> <li>Daily: Keep the most recent snapshot from each day</li> <li>Weekly: Keep the most recent snapshot from each week</li> <li>Monthly: Keep the most recent snapshot from each month</li> <li>Yearly: Keep the most recent snapshot from each year</li> </ul>"},{"location":"expiry-policy/#how-it-works","title":"How It Works","text":"<p>When applying an expiry policy, FlashFS:</p> <ol> <li>Sorts all snapshots by timestamp (newest first)</li> <li>Identifies snapshots to keep based on retention policies</li> <li>Applies maximum snapshots limit (if configured)</li> <li>Applies maximum age limit (if configured)</li> <li>Deletes snapshots that don't meet the retention criteria</li> </ol> <p>The policy is automatically applied when creating new snapshots, or can be manually applied using the <code>expiry apply</code> command.</p>"},{"location":"expiry-policy/#command-reference","title":"Command Reference","text":""},{"location":"expiry-policy/#setting-an-expiry-policy","title":"Setting an Expiry Policy","text":"<pre><code>flashfs expiry set [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--max-snapshots &lt;number&gt;</code>: Maximum number of snapshots to keep (0 = unlimited)</li> <li><code>--max-age &lt;duration&gt;</code>: Maximum age of snapshots to keep (e.g., 30d, 2w, 6m, 1y)</li> <li><code>--keep-hourly &lt;number&gt;</code>: Number of hourly snapshots to keep</li> <li><code>--keep-daily &lt;number&gt;</code>: Number of daily snapshots to keep</li> <li><code>--keep-weekly &lt;number&gt;</code>: Number of weekly snapshots to keep</li> <li><code>--keep-monthly &lt;number&gt;</code>: Number of monthly snapshots to keep</li> <li><code>--keep-yearly &lt;number&gt;</code>: Number of yearly snapshots to keep</li> <li><code>--apply</code>: Apply the policy immediately after setting it</li> <li><code>--dir &lt;path&gt;</code>: Base directory for snapshots (defaults to current directory)</li> </ul>"},{"location":"expiry-policy/#applying-an-expiry-policy","title":"Applying an Expiry Policy","text":"<pre><code>flashfs expiry apply [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--dir &lt;path&gt;</code>: Base directory for snapshots (defaults to current directory)</li> </ul>"},{"location":"expiry-policy/#showing-the-current-expiry-policy","title":"Showing the Current Expiry Policy","text":"<pre><code>flashfs expiry show [options]\n</code></pre> <p>Options:</p> <ul> <li><code>--dir &lt;path&gt;</code>: Base directory for snapshots (defaults to current directory)</li> </ul>"},{"location":"expiry-policy/#examples","title":"Examples","text":""},{"location":"expiry-policy/#basic-retention-policy","title":"Basic Retention Policy","text":"<p>Keep the 10 most recent snapshots:</p> <pre><code>flashfs expiry set --max-snapshots 10\n</code></pre>"},{"location":"expiry-policy/#age-based-cleanup","title":"Age-Based Cleanup","text":"<p>Remove snapshots older than 30 days:</p> <pre><code>flashfs expiry set --max-age 30d\n</code></pre>"},{"location":"expiry-policy/#comprehensive-backup-strategy","title":"Comprehensive Backup Strategy","text":"<p>Implement a comprehensive backup strategy with different retention periods:</p> <pre><code>flashfs expiry set --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 12 --keep-yearly 5\n</code></pre> <p>This will keep:</p> <ul> <li>24 hourly snapshots (one per hour for the last day)</li> <li>7 daily snapshots (one per day for the last week)</li> <li>4 weekly snapshots (one per week for the last month)</li> <li>12 monthly snapshots (one per month for the last year)</li> <li>5 yearly snapshots (one per year for the last five years)</li> </ul>"},{"location":"expiry-policy/#combined-policy","title":"Combined Policy","text":"<p>Combine different policy types for comprehensive management:</p> <pre><code>flashfs expiry set --max-snapshots 100 --max-age 365d --keep-hourly 24 --keep-daily 7\n</code></pre> <p>This will:</p> <ol> <li>Apply the hourly and daily retention policies</li> <li>Ensure no more than 100 snapshots are kept in total</li> <li>Remove any snapshots older than 365 days</li> </ol>"},{"location":"expiry-policy/#immediate-application","title":"Immediate Application","text":"<p>Set a policy and apply it immediately:</p> <pre><code>flashfs expiry set --max-age 30d --apply\n</code></pre>"},{"location":"expiry-policy/#implementation-details","title":"Implementation Details","text":"<p>The expiry policy is implemented in the <code>SnapshotStore</code> struct and is persisted between sessions. When a new snapshot is created, the policy is automatically applied to clean up old snapshots according to the configured rules.</p> <p>The policy implementation prioritizes retention policies (hourly, daily, etc.) before applying maximum snapshots and maximum age limits. This ensures that important historical snapshots are preserved even when limits are applied.</p>"},{"location":"hash/","title":"Hash Module","text":"<p>The Hash module provides efficient and flexible file hashing capabilities for FlashFS. It supports multiple hashing algorithms, concurrent processing, and memory-efficient operations through a buffer pool.</p>"},{"location":"hash/#overview","title":"Overview","text":"<p>The Hash module is designed to:</p> <ol> <li>Compute hashes for files, byte slices, strings, and readers</li> <li>Support multiple hashing algorithms (BLAKE3, MD5, SHA1, SHA256)</li> <li>Optimize memory usage with a buffer pool</li> <li>Enable concurrent hashing of multiple files</li> <li>Provide partial hashing for large files to improve performance</li> </ol>"},{"location":"hash/#key-components","title":"Key Components","text":""},{"location":"hash/#algorithms","title":"Algorithms","text":"<p>The module supports the following hashing algorithms:</p> <ul> <li>BLAKE3 (default): Fast and cryptographically secure</li> <li>MD5: Provided for compatibility (not cryptographically secure)</li> <li>SHA1: Provided for compatibility (not cryptographically secure)</li> <li>SHA256: Secure but slower algorithm</li> </ul>"},{"location":"hash/#options","title":"Options","text":"<p>Hashing behavior can be configured using the <code>Options</code> struct:</p> <pre><code>type Options struct {\n    Algorithm               Algorithm\n    BufferSize              int\n    SkipErrors              bool\n    Concurrency             int\n    UsePartialHashing       bool\n    PartialHashingThreshold int64\n}\n</code></pre> <ul> <li>Algorithm: The hashing algorithm to use</li> <li>BufferSize: Size of the buffer used for reading files (default: 16MB)</li> <li>SkipErrors: Whether to return an error or an empty hash on failure</li> <li>Concurrency: Number of files to hash concurrently (0 = use all available CPUs)</li> <li>UsePartialHashing: Enables partial hashing for large files</li> <li>PartialHashingThreshold: File size threshold above which partial hashing is used (default: 10MB)</li> </ul>"},{"location":"hash/#result","title":"Result","text":"<p>The <code>Result</code> struct represents the outcome of a hashing operation:</p> <pre><code>type Result struct {\n    Hash      string\n    Error     error\n    Algorithm Algorithm\n    Size      int64\n}\n</code></pre>"},{"location":"hash/#core-functions","title":"Core Functions","text":""},{"location":"hash/#file-hashing","title":"File Hashing","text":"<pre><code>// Hash a file using the specified options\nresult := hash.File(path, options)\n\n// Hash multiple files concurrently\nresults := hash.FilesConcurrent(paths, options)\n\n// Hash a large file using partial hashing\nresult := hash.PartialFile(path, options)\n</code></pre>"},{"location":"hash/#other-hashing-functions","title":"Other Hashing Functions","text":"<pre><code>// Hash a byte slice\nresult := hash.Bytes(data, algorithm)\n\n// Hash a string\nresult := hash.String(data, algorithm)\n\n// Hash data from an io.Reader\nresult := hash.Reader(reader, algorithm)\n\n// Verify a file against an expected hash\nmatch, err := hash.Verify(path, expectedHash, options)\n</code></pre>"},{"location":"hash/#partial-hashing","title":"Partial Hashing","text":"<p>For large files, the module provides a partial hashing feature that samples portions of the file rather than reading the entire content:</p> <ul> <li>The first N bytes</li> <li>The middle N bytes</li> <li>The last N bytes</li> </ul> <p>These samples are combined to create a representative hash of the file. This approach is significantly faster for very large files while still providing reliable change detection.</p> <pre><code>// Enable partial hashing in options\noptions := hash.DefaultOptions()\noptions.UsePartialHashing = true\noptions.PartialHashingThreshold = 100 * 1024 * 1024 // 100MB\n\n// Hash a file (will use partial hashing for files &gt; 100MB)\nresult := hash.File(path, options)\n</code></pre>"},{"location":"hash/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Buffer Size: Larger buffers generally improve throughput but consume more memory</li> <li>Concurrency: Adjust based on available CPU cores and I/O capabilities</li> <li>Partial Hashing: Consider for very large files where full hashing would be prohibitively expensive</li> <li>Algorithm Choice: BLAKE3 offers the best performance among the secure algorithms</li> </ul>"},{"location":"hash/#example-usage","title":"Example Usage","text":"<pre><code>// Basic file hashing with default options\nresult := hash.File(\"path/to/file.txt\", hash.DefaultOptions())\nif result.Error != nil {\n    log.Fatalf(\"Failed to hash file: %v\", result.Error)\n}\nfmt.Printf(\"Hash: %s\\n\", result.Hash)\n\n// Concurrent hashing of multiple files\nfiles := []string{\"file1.txt\", \"file2.txt\", \"file3.txt\"}\noptions := hash.DefaultOptions()\noptions.Algorithm = hash.SHA256\nresults := hash.FilesConcurrent(files, options)\nfor path, result := range results {\n    if result.Error != nil {\n        fmt.Printf(\"Failed to hash %s: %v\\n\", path, result.Error)\n        continue\n    }\n    fmt.Printf(\"%s: %s\\n\", path, result.Hash)\n}\n</code></pre>"},{"location":"release-v0.1.0/","title":"FlashFS v0.1.0 Release Notes","text":"<p>FlashFS is a high-performance file system snapshot and diff tool designed for efficiently tracking and managing changes across file systems over time.</p>"},{"location":"release-v0.1.0/#overview","title":"Overview","text":"<p>FlashFS v0.1.0 provides a robust foundation for creating, comparing, and managing file system snapshots with a focus on performance and efficiency. This release introduces the core functionality and architecture that will serve as the basis for future enhancements.</p>"},{"location":"release-v0.1.0/#key-features","title":"Key Features","text":""},{"location":"release-v0.1.0/#snapshot-management","title":"Snapshot Management","text":"<ul> <li>Fast Snapshot Creation: Efficiently capture the state of your file system with parallel processing</li> <li>Content-Based Deduplication: Use BLAKE3 hashing for accurate file identification</li> <li>Configurable Compression: Balance speed and size with zstd compression</li> <li>Intelligent Caching: Optimize performance for frequently accessed snapshots</li> </ul>"},{"location":"release-v0.1.0/#diff-computation","title":"Diff Computation","text":"<ul> <li>Structured Diff Format: Clear representation of added, modified, and deleted files</li> <li>Bloom Filter Pre-filtering: Quickly identify potential changes between snapshots</li> <li>Parallel Processing: Distribute comparison work across multiple CPU cores</li> <li>Path Filtering: Focus comparisons on specific directories or file patterns</li> </ul>"},{"location":"release-v0.1.0/#snapshot-lifecycle-management","title":"Snapshot Lifecycle Management","text":"<ul> <li>Flexible Expiry Policies: Configure retention based on time periods and snapshot counts</li> <li>Time-Based Retention: Keep hourly, daily, weekly, monthly, and yearly snapshots</li> <li>Count-Based Retention: Limit the total number of snapshots</li> <li>Age-Based Cleanup: Automatically remove snapshots older than a specified age</li> </ul>"},{"location":"release-v0.1.0/#query-capabilities","title":"Query Capabilities","text":"<ul> <li>Path Pattern Matching: Find files matching specific patterns</li> <li>Metadata Filtering: Filter by modification time, size, and file type</li> <li>Multiple Output Formats: Export results as text, JSON, or CSV</li> </ul>"},{"location":"release-v0.1.0/#technical-highlights","title":"Technical Highlights","text":"<ul> <li>Efficient Binary Serialization: Using FlatBuffers for compact representation and fast access</li> <li>Minimal Memory Usage: Streaming processing for handling large file systems</li> <li>Thread Safety: Concurrent operations with proper synchronization</li> <li>Comprehensive CLI: Full-featured command-line interface built with Cobra</li> </ul>"},{"location":"release-v0.1.0/#documentation","title":"Documentation","text":"<p>Comprehensive documentation is available at https://TFMV.github.io/flashfs/, including:</p> <ul> <li>Component overviews</li> <li>Command references</li> <li>Usage examples</li> <li>Implementation details</li> <li>Performance considerations</li> </ul>"},{"location":"release-v0.1.0/#installation","title":"Installation","text":"<pre><code>go install github.com/TFMV/flashfs@latest\n</code></pre>"},{"location":"release-v0.1.0/#example-usage","title":"Example Usage","text":"<p>Create a snapshot:</p> <pre><code>flashfs snapshot --path /home/user/documents --output backup-20240601.snap\n</code></pre> <p>Compute differences:</p> <pre><code>flashfs diff --base backup-20240601.snap --target backup-20240602.snap --output changes.diff\n</code></pre> <p>Apply a diff:</p> <pre><code>flashfs apply --base backup-20240601.snap --diff changes.diff --output backup-20240602.snap\n</code></pre> <p>Set an expiry policy:</p> <pre><code>flashfs expiry set --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 12\n</code></pre>"},{"location":"release-v0.1.0/#known-limitations","title":"Known Limitations","text":"<ul> <li>Limited support for special file types (devices, sockets, etc.)</li> <li>No built-in encryption for snapshots and diffs</li> <li>No remote storage backend integration</li> </ul>"},{"location":"release-v0.1.0/#future-roadmap","title":"Future Roadmap","text":"<ul> <li>Encryption support for secure storage</li> <li>Remote storage backends (S3, GCS, etc.)</li> <li>GUI for visualization and management</li> <li>Improved handling of special file types</li> <li>Snapshot verification and integrity checking</li> </ul>"},{"location":"release-v0.1.0/#contributors","title":"Contributors","text":"<ul> <li>Thomas F McGeehan V (Lead Developer)</li> </ul>"},{"location":"release-v0.1.0/#license","title":"License","text":"<p>FlashFS is released under the MIT License.</p> <p>We welcome feedback, bug reports, and contributions! Please file issues on our GitHub repository.</p>"},{"location":"schema/","title":"Schema","text":"<p>The Schema component in FlashFS defines the data structures used for serialization and deserialization of snapshots and diffs. It uses FlatBuffers to create an efficient, cross-platform binary representation of file system metadata.</p>"},{"location":"schema/#overview","title":"Overview","text":"<p>The Schema component:</p> <ul> <li>Defines the structure of snapshots and diffs</li> <li>Enables efficient binary serialization</li> <li>Provides zero-copy access to serialized data</li> <li>Ensures cross-platform compatibility</li> <li>Allows for schema evolution over time</li> </ul>"},{"location":"schema/#flatbuffers-schema-definition","title":"FlatBuffers Schema Definition","text":"<p>FlashFS uses FlatBuffers as its serialization framework. The schema is defined in the FlatBuffers Interface Definition Language (IDL):</p> <pre><code>namespace flashfs;\n\ntable FileEntry {\n  path: string;\n  size: long;\n  mtime: long;\n  isDir: bool;\n  permissions: uint;\n  hash: [ubyte];\n}\n\ntable Snapshot {\n  entries: [FileEntry];\n}\n\ntable DiffEntry {\n  path: string;\n  type: byte;  // 0 = added, 1 = modified, 2 = deleted\n  oldSize: long;\n  newSize: long;\n  oldMtime: long;\n  newMtime: long;\n  oldPermissions: uint;\n  newPermissions: uint;\n  oldHash: [ubyte];\n  newHash: [ubyte];\n}\n\ntable Diff {\n  entries: [DiffEntry];\n}\n\nroot_type Snapshot;\n</code></pre>"},{"location":"schema/#schema-components","title":"Schema Components","text":""},{"location":"schema/#fileentry","title":"FileEntry","text":"<p>The <code>FileEntry</code> table represents a single file or directory in a snapshot:</p> <ul> <li>path: The relative path within the snapshot</li> <li>size: File size in bytes (0 for directories)</li> <li>mtime: Modification time as Unix timestamp</li> <li>isDir: Whether the entry is a directory</li> <li>permissions: File permissions as a uint32</li> <li>hash: BLAKE3 hash of the file contents (empty for directories)</li> </ul>"},{"location":"schema/#snapshot","title":"Snapshot","text":"<p>The <code>Snapshot</code> table contains a list of file entries that represent the state of a file system at a specific point in time.</p>"},{"location":"schema/#diffentry","title":"DiffEntry","text":"<p>The <code>DiffEntry</code> table represents a change between two snapshots:</p> <ul> <li>path: The path of the changed file or directory</li> <li>type: The type of change (added, modified, deleted)</li> <li>oldSize/newSize: File size before and after the change</li> <li>oldMtime/newMtime: Modification time before and after the change</li> <li>oldPermissions/newPermissions: Permissions before and after the change</li> <li>oldHash/newHash: Content hash before and after the change</li> </ul>"},{"location":"schema/#diff","title":"Diff","text":"<p>The <code>Diff</code> table contains a list of diff entries that represent the changes between two snapshots.</p>"},{"location":"schema/#generated-code","title":"Generated Code","text":"<p>The FlatBuffers compiler generates Go code from the schema definition:</p> <pre><code>// Generated code provides type-safe accessors for all fields\nfunc (e *FileEntry) Path() string\nfunc (e *FileEntry) Size() int64\nfunc (e *FileEntry) Mtime() int64\nfunc (e *FileEntry) IsDir() bool\nfunc (e *FileEntry) Permissions() uint32\nfunc (e *FileEntry) Hash(j int) byte\nfunc (e *FileEntry) HashLength() int\n</code></pre>"},{"location":"schema/#usage-examples","title":"Usage Examples","text":""},{"location":"schema/#accessing-snapshot-data","title":"Accessing Snapshot Data","text":"<pre><code>// Deserialize a snapshot\nsnapshot := flashfs.GetRootAsSnapshot(serializedData, 0)\n\n// Get the number of entries\nentriesLength := snapshot.EntriesLength()\n\n// Access individual entries\nfor i := 0; i &lt; entriesLength; i++ {\n    var entry flashfs.FileEntry\n    if snapshot.Entries(&amp;entry, i) {\n        fmt.Printf(\"Path: %s, Size: %d bytes\\n\", entry.Path(), entry.Size())\n\n        // Access hash bytes if available\n        if entry.HashLength() &gt; 0 {\n            hash := make([]byte, entry.HashLength())\n            for j := 0; j &lt; entry.HashLength(); j++ {\n                hash[j] = entry.Hash(j)\n            }\n            fmt.Printf(\"Hash: %x\\n\", hash)\n        }\n    }\n}\n</code></pre>"},{"location":"schema/#creating-serialized-data","title":"Creating Serialized Data","text":"<pre><code>// Create a FlatBuffers builder\nbuilder := flatbuffers.NewBuilder(0)\n\n// Create file entries\nfileEntryOffsets := make([]flatbuffers.UOffsetT, len(entries))\nfor i, entry := range entries {\n    // Create string and byte vector offsets\n    pathOffset := builder.CreateString(entry.Path)\n    hashOffset := builder.CreateByteVector(entry.Hash)\n\n    // Start building a FileEntry\n    flashfs.FileEntryStart(builder)\n    flashfs.FileEntryAddPath(builder, pathOffset)\n    flashfs.FileEntryAddSize(builder, entry.Size)\n    flashfs.FileEntryAddMtime(builder, entry.ModTime)\n    flashfs.FileEntryAddIsDir(builder, entry.IsDir)\n    flashfs.FileEntryAddPermissions(builder, entry.Permissions)\n    flashfs.FileEntryAddHash(builder, hashOffset)\n\n    // Finish the FileEntry\n    fileEntryOffsets[i] = flashfs.FileEntryEnd(builder)\n}\n\n// Create a vector of file entries\nflashfs.SnapshotStartEntriesVector(builder, len(fileEntryOffsets))\nfor i := len(fileEntryOffsets) - 1; i &gt;= 0; i-- {\n    builder.PrependUOffsetT(fileEntryOffsets[i])\n}\nentriesVector := builder.EndVector(len(fileEntryOffsets))\n\n// Create the snapshot\nflashfs.SnapshotStart(builder)\nflashfs.SnapshotAddEntries(builder, entriesVector)\nsnapshot := flashfs.SnapshotEnd(builder)\n\n// Finish the builder\nbuilder.Finish(snapshot)\n\n// Get the serialized data\nserializedData := builder.FinishedBytes()\n</code></pre>"},{"location":"schema/#schema-evolution","title":"Schema Evolution","text":"<p>The FlatBuffers schema allows for backward-compatible evolution:</p> <ul> <li>Adding Fields: New fields can be added without breaking compatibility with older data</li> <li>Deprecating Fields: Fields can be deprecated without breaking compatibility</li> <li>Versioning: Schema versioning can be implemented through optional fields</li> </ul>"},{"location":"schema/#performance-considerations","title":"Performance Considerations","text":"<p>The Schema component is designed for high performance:</p> <ul> <li>Zero-Copy Deserialization: Accessing data doesn't require unpacking or parsing</li> <li>Memory Efficiency: Binary representation is compact and memory-efficient</li> <li>Cross-Platform: Same binary format works across different platforms</li> <li>Fast Access: Direct access to fields without traversing the entire structure</li> </ul>"},{"location":"schema/#integration-with-other-components","title":"Integration with Other Components","text":"<p>The Schema component integrates with:</p> <ul> <li>Serializer: Provides the structure for serialization</li> <li>Storage: Defines the format for stored snapshots and diffs</li> <li>Diff: Enables efficient representation of changes</li> </ul>"},{"location":"schema/#advanced-topics","title":"Advanced Topics","text":""},{"location":"schema/#custom-schemas","title":"Custom Schemas","text":"<p>For specialized use cases, the schema can be extended:</p> <pre><code>// Extended schema with additional metadata\ntable SnapshotWithMetadata {\n  entries: [FileEntry];\n  creationTime: long;\n  description: string;\n  tags: [string];\n}\n</code></pre>"},{"location":"schema/#schema-versioning","title":"Schema Versioning","text":"<p>To maintain compatibility across versions:</p> <pre><code>// Versioned schema\ntable SnapshotV2 {\n  entries: [FileEntry];\n  version: uint = 2;  // Default value for backward compatibility\n  compressionType: byte = 0;  // New field in version 2\n}\n</code></pre>"},{"location":"schema/#nested-structures","title":"Nested Structures","text":"<p>For more complex data representation:</p> <pre><code>// Nested structures\ntable Directory {\n  path: string;\n  files: [FileEntry];\n  subdirectories: [Directory];\n}\n\ntable HierarchicalSnapshot {\n  rootDirectory: Directory;\n}\n</code></pre>"},{"location":"serializer/","title":"Serializer","text":"<p>The serializer package provides efficient serialization and deserialization of FlashFS data structures, particularly snapshots and diffs. It uses FlatBuffers as the underlying serialization format, which offers memory-efficient zero-copy deserialization.</p>"},{"location":"serializer/#features","title":"Features","text":"<ul> <li>Memory Efficiency: Uses FlatBuffers for zero-copy deserialization</li> <li>Builder Reuse: Optimizes memory usage by reusing FlatBuffer builders</li> <li>Streaming Support: Handles large datasets that don't fit in memory</li> <li>Progress Tracking: Provides chunk-based progress reporting</li> <li>Parallel Processing: Supports concurrent processing of chunks</li> </ul>"},{"location":"serializer/#core-components","title":"Core Components","text":""},{"location":"serializer/#standard-serialization","title":"Standard Serialization","text":"<p>The standard serialization functions handle snapshots and diffs that fit in memory:</p> <ul> <li><code>SerializeSnapshotFB</code>: Serializes a snapshot to a byte array</li> <li><code>DeserializeSnapshotFB</code>: Deserializes a snapshot from a byte array</li> <li><code>SerializeDiffFB</code>: Serializes a diff to a byte array</li> <li><code>DeserializeDiffFB</code>: Deserializes a diff from a byte array</li> </ul>"},{"location":"serializer/#streaming-serialization","title":"Streaming Serialization","text":"<p>For large datasets, the streaming serialization components break data into manageable chunks:</p> <ul> <li><code>StreamingSerializer</code>: Serializes and deserializes snapshots in chunks</li> <li><code>StreamingDiffSerializer</code>: Serializes and deserializes diffs in chunks</li> </ul>"},{"location":"serializer/#memory-optimization","title":"Memory Optimization","text":""},{"location":"serializer/#builder-reuse","title":"Builder Reuse","text":"<p>The serializer supports reusing FlatBuffers builders to reduce memory allocations:</p> <pre><code>// Create a builder once\nbuilder := flatbuffers.NewBuilder(0)\n\n// Reuse it for multiple serializations\nfor _, snapshot := range snapshots {\n    data, err := serializer.SerializeSnapshotFB(snapshot, builder)\n    // Process data...\n}\n</code></pre>"},{"location":"serializer/#builder-pool","title":"Builder Pool","text":"<p>When passing <code>nil</code> as the builder parameter, the serializer automatically uses a builder pool:</p> <pre><code>// This will get a builder from the pool and return it when done\ndata, err := serializer.SerializeSnapshotFB(entries, nil)\n</code></pre>"},{"location":"serializer/#streaming-serialization_1","title":"Streaming Serialization","text":""},{"location":"serializer/#configuration","title":"Configuration","text":"<p>The streaming serializer can be configured with options:</p> <pre><code>options := serializer.StreamingOptions{\n    ChunkSize:  5000,     // 5000 entries per chunk\n    BufferSize: 128*1024, // 128KB buffer\n}\nserializer := serializer.NewStreamingSerializer(options)\n</code></pre>"},{"location":"serializer/#writing-to-storage","title":"Writing to Storage","text":"<pre><code>// Serialize to a writer (file, network, etc.)\nerr := serializer.SerializeToWriter(entries, writer)\n</code></pre>"},{"location":"serializer/#reading-from-storage","title":"Reading from Storage","text":"<pre><code>// Deserialize from a reader with a callback for each chunk\nerr := serializer.DeserializeFromReader(reader, func(chunk SnapshotChunk) error {\n    // Process chunk.Entries\n    // Track progress with chunk.Index and chunk.Total\n    return nil\n})\n</code></pre>"},{"location":"serializer/#progress-tracking","title":"Progress Tracking","text":"<p>The streaming serializer provides progress information through chunk metadata:</p> <pre><code>err := serializer.DeserializeFromReader(reader, func(chunk SnapshotChunk) error {\n    // Update progress\n    progress := float64(chunk.Index) / float64(chunk.Total) * 100\n    fmt.Printf(\"Progress: %.2f%% (Chunk %d/%d)\\n\", \n        progress, chunk.Index, chunk.Total)\n    return nil\n})\n</code></pre>"},{"location":"serializer/#parallel-processing","title":"Parallel Processing","text":"<p>Chunks can be processed in parallel for improved performance:</p> <pre><code>// Create worker pool\nvar wg sync.WaitGroup\nchunkChan := make(chan SnapshotChunk, 10)\n\n// Start worker goroutines\nfor i := 0; i &lt; runtime.NumCPU(); i++ {\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        for chunk := range chunkChan {\n            // Process chunk in parallel\n            processChunk(chunk)\n        }\n    }()\n}\n\n// Feed chunks to workers\nserializer.DeserializeFromReader(reader, func(chunk SnapshotChunk) error {\n    chunkChan &lt;- chunk\n    return nil\n})\nclose(chunkChan)\n\n// Wait for all workers to finish\nwg.Wait()\n</code></pre>"},{"location":"serializer/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Builder Reuse: Always reuse builders for multiple serializations to reduce allocations.</p> </li> <li> <p>Chunk Size: For streaming serialization, adjust the chunk size based on your memory constraints and processing needs. Larger chunks are more efficient but use more memory.</p> </li> <li> <p>Buffer Size: The buffer size affects I/O performance. Larger buffers generally improve performance but use more memory.</p> </li> <li> <p>Parallel Processing: For large datasets, consider processing chunks in parallel during deserialization.</p> </li> </ol>"},{"location":"serializer/#use-cases","title":"Use Cases","text":""},{"location":"serializer/#efficient-storage","title":"Efficient Storage","text":"<p>The serializer is optimized for efficient storage of filesystem metadata:</p> <ul> <li>Snapshots: Store the state of a filesystem at a point in time</li> <li>Diffs: Store the changes between two snapshots</li> </ul>"},{"location":"serializer/#network-transfer","title":"Network Transfer","text":"<p>The streaming serializer is ideal for transferring large datasets over a network:</p> <ul> <li>Chunked Transfer: Send data in manageable chunks</li> <li>Progress Reporting: Track transfer progress</li> <li>Resumable Transfers: Potentially resume interrupted transfers</li> </ul>"},{"location":"serializer/#large-dataset-processing","title":"Large Dataset Processing","text":"<p>For very large filesystems, the streaming serializer enables processing that wouldn't fit in memory:</p> <ul> <li>Incremental Processing: Process data as it becomes available</li> <li>Parallel Processing: Process chunks concurrently</li> <li>Memory Efficiency: Control memory usage with chunk size</li> </ul>"},{"location":"storage/","title":"Storage","text":"<p>The Storage component is the core of FlashFS, responsible for managing snapshots, diffs, and providing efficient access to stored data. It handles compression, caching, and lifecycle management of snapshots.</p>"},{"location":"storage/#overview","title":"Overview","text":"<p>The Storage component provides a robust API for:</p> <ul> <li>Writing and reading snapshots</li> <li>Computing and applying diffs between snapshots</li> <li>Querying snapshot contents</li> <li>Managing snapshot lifecycle through expiry policies</li> <li>Optimizing performance through caching and Bloom filters</li> </ul>"},{"location":"storage/#snapshotstore","title":"SnapshotStore","text":"<p>The primary interface to the Storage component is the <code>SnapshotStore</code> struct:</p> <pre><code>type SnapshotStore struct {\n    baseDir       string\n    encoder       *zstd.Encoder\n    decoder       *zstd.Decoder\n    cacheMutex    sync.RWMutex\n    snapshotCache map[string][]byte\n    cacheSize     int\n    cacheKeys     []string\n    expiryPolicy  ExpiryPolicy\n}\n</code></pre>"},{"location":"storage/#initialization","title":"Initialization","text":"<p>Create a new <code>SnapshotStore</code> with:</p> <pre><code>store, err := storage.NewSnapshotStore(\"/path/to/snapshots\")\nif err != nil {\n    return err\n}\ndefer store.Close()\n</code></pre> <p>This initializes a store with:</p> <ul> <li>Default compression level (Zstd level 3)</li> <li>Default cache size (10 snapshots)</li> <li>Default expiry policy</li> </ul>"},{"location":"storage/#core-functions","title":"Core Functions","text":""},{"location":"storage/#snapshot-management","title":"Snapshot Management","text":"<pre><code>// Create a new snapshot\nerr := store.CreateSnapshot(\"backup-20230101.snap\", snapshotData)\n\n// Read a snapshot\ndata, err := store.ReadSnapshot(\"backup-20230101.snap\")\n\n// List all snapshots\nsnapshots, err := store.ListSnapshots()\n\n// Delete a snapshot\nerr := store.DeleteSnapshot(\"backup-20230101.snap\")\n</code></pre>"},{"location":"storage/#diff-operations","title":"Diff Operations","text":"<pre><code>// Compute a diff between two snapshots\n// Returns a serialized Diff object containing DiffEntry items\ndiffData, err := store.ComputeDiff(\"backup-20230101.snap\", \"backup-20230102.snap\")\n\n// Store a diff to a file\nerr := store.StoreDiff(\"backup-20230101.snap\", \"backup-20230102.snap\")\n\n// Apply a diff to generate a new snapshot\n// Processes each DiffEntry based on its type (added, modified, deleted)\nnewSnapshotData, err := store.ApplyDiff(\"backup-20230101.snap\", \"diff-20230101-20230102.diff\")\n</code></pre> <p>The diff operations work with structured Diff and DiffEntry types:</p> <pre><code>// DiffEntry represents a single changed file\n// type field indicates: 0 = added, 1 = modified, 2 = deleted\ntype DiffEntry struct {\n    path           string\n    type           byte\n    oldSize        int64\n    newSize        int64\n    oldMtime       int64\n    newMtime       int64\n    oldPermissions uint32\n    newPermissions uint32\n    oldHash        []byte\n    newHash        []byte\n}\n\n// Diff contains all changes between two snapshots\ntype Diff struct {\n    entries []DiffEntry\n}\n</code></pre> <p>The <code>ComputeDiff</code> function compares two snapshots and creates a structured diff with:</p> <ul> <li>Added files (type = 0)</li> <li>Modified files (type = 1)</li> <li>Deleted files (type = 2)</li> </ul> <p>The <code>ApplyDiff</code> function processes each DiffEntry based on its type to:</p> <ul> <li>Add new files to the base snapshot</li> <li>Update modified files in the base snapshot</li> <li>Remove deleted files from the base snapshot</li> </ul>"},{"location":"storage/#querying","title":"Querying","text":"<pre><code>// Query snapshot contents with a filter function\nentries, err := store.QuerySnapshot(\"backup-20230101.snap\", func(entry *flashfs.FileEntry) bool {\n    // Return true for entries to include\n    return strings.HasPrefix(entry.Path(), \"/home/user/documents/\")\n})\n</code></pre>"},{"location":"storage/#expiry-policy-management","title":"Expiry Policy Management","text":"<pre><code>// Set an expiry policy\nstore.SetExpiryPolicy(storage.ExpiryPolicy{\n    MaxSnapshots: 50,\n    MaxAge:       30 * 24 * time.Hour,\n    KeepHourly:   24,\n    KeepDaily:    7,\n})\n\n// Get the current expiry policy\npolicy := store.GetExpiryPolicy()\n\n// Apply the expiry policy to clean up old snapshots\ndeleted, err := store.ApplyExpiryPolicy()\n</code></pre>"},{"location":"storage/#caching","title":"Caching","text":"<p>The Storage component implements an LRU (Least Recently Used) cache for snapshots:</p> <pre><code>// Set the cache size\nstore.SetCacheSize(20)\n\n// The cache is automatically managed when reading snapshots\ndata, err := store.ReadSnapshot(\"backup-20230101.snap\") // Automatically cached\n\n// Subsequent reads of the same snapshot will use the cached version\ndata, err = store.ReadSnapshot(\"backup-20230101.snap\") // Retrieved from cache\n</code></pre>"},{"location":"storage/#bloom-filters","title":"Bloom Filters","text":"<p>Bloom filters are used to quickly identify changed files without full snapshot comparison:</p> <pre><code>// Create a Bloom filter from a snapshot\nfilter, err := store.CreateBloomFilterFromSnapshot(\"backup-20230101.snap\")\n\n// Use the filter to check if a file might have changed\nmightContain := filter.Contains([]byte(\"/path/to/file\"))\n</code></pre>"},{"location":"storage/#file-format","title":"File Format","text":""},{"location":"storage/#snapshots","title":"Snapshots","text":"<p>Snapshots are stored with the <code>.snap</code> extension and contain:</p> <ol> <li>Serialized file metadata in FlatBuffers format</li> <li>Compressed using Zstd</li> </ol>"},{"location":"storage/#diffs","title":"Diffs","text":"<p>Diffs are stored with the <code>.diff</code> extension and contain:</p> <ol> <li>Serialized Diff object with DiffEntry items for added, modified, and deleted files</li> <li>Each DiffEntry contains a type field and relevant metadata for the change</li> <li>Compressed using Zstd</li> </ol>"},{"location":"storage/#performance-considerations","title":"Performance Considerations","text":"<p>The Storage component is optimized for:</p> <ul> <li>Read Performance: Uses caching to minimize disk I/O for frequently accessed snapshots</li> <li>Write Performance: Uses efficient compression to balance speed and size</li> <li>Memory Usage: Controls memory consumption through configurable cache size</li> <li>Comparison Speed: Uses Bloom filters to accelerate diff operations</li> <li>Diff Processing: Structured diff format enables efficient application of changes</li> </ul>"},{"location":"storage/#implementation-details","title":"Implementation Details","text":""},{"location":"storage/#compression","title":"Compression","text":"<p>FlashFS uses Zstd compression with configurable levels:</p> <ul> <li>Default level: 3 (good balance of speed and compression ratio)</li> <li>Faster compression: Use lower levels (1-2)</li> <li>Better compression ratio: Use higher levels (4-9)</li> </ul>"},{"location":"storage/#thread-safety","title":"Thread Safety","text":"<p>The Storage component is thread-safe:</p> <ul> <li>Uses read/write mutexes for cache access</li> <li>Safe for concurrent reads</li> <li>Serializes writes to prevent corruption</li> </ul>"},{"location":"storage/#error-handling","title":"Error Handling","text":"<p>The Storage component provides detailed error information:</p> <ul> <li>File not found errors</li> <li>Permission errors</li> <li>Corruption detection</li> <li>Version incompatibility</li> </ul>"},{"location":"storage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"storage/#custom-compression","title":"Custom Compression","text":"<pre><code>// Create a store with custom compression level\noptions := storage.Options{\n    CompressionLevel: 9, // Maximum compression\n}\nstore, err := storage.NewSnapshotStoreWithOptions(\"/path/to/snapshots\", options)\n</code></pre>"},{"location":"storage/#snapshot-metadata","title":"Snapshot Metadata","text":"<pre><code>// Get metadata about snapshots without loading full contents\nmetadata, err := store.GetSnapshotMetadata(\"backup-20230101.snap\")\nfmt.Printf(\"Size: %d bytes, Created: %s\\n\", metadata.Size, metadata.Timestamp)\n</code></pre>"},{"location":"streaming/","title":"Streaming Processing","text":"<p>FlashFS implements streaming processing capabilities throughout its architecture to efficiently handle very large datasets that wouldn't fit entirely in memory. This approach enables processing of massive file systems with minimal memory footprint while maintaining high performance.</p>"},{"location":"streaming/#core-streaming-components","title":"Core Streaming Components","text":"<p>FlashFS implements streaming in several key components:</p>"},{"location":"streaming/#streaming-directory-walker","title":"Streaming Directory Walker","text":"<p>The directory walker processes files as they're discovered, rather than collecting all files before processing:</p> <ul> <li>Memory Efficiency: Only keeps necessary metadata in memory</li> <li>Responsiveness: Provides immediate feedback as files are processed</li> <li>Parallel Processing: Can process multiple directories concurrently</li> </ul>"},{"location":"streaming/#streaming-serialization","title":"Streaming Serialization","text":"<p>The serializer breaks large datasets into manageable chunks:</p> <ul> <li>Chunked Processing: Divides data into fixed-size chunks</li> <li>Progress Tracking: Reports progress as chunks are processed</li> <li>Parallel Processing: Enables concurrent processing of chunks</li> </ul>"},{"location":"streaming/#streaming-diff-computation","title":"Streaming Diff Computation","text":"<p>The diff engine can compute differences between snapshots in a streaming fashion:</p> <ul> <li>Incremental Comparison: Compares entries as they're read</li> <li>Memory-Efficient: Doesn't require loading entire snapshots into memory</li> <li>Early Termination: Can stop processing when specific conditions are met</li> </ul>"},{"location":"streaming/#benefits-of-streaming","title":"Benefits of Streaming","text":""},{"location":"streaming/#memory-efficiency","title":"Memory Efficiency","text":"<p>Streaming processing dramatically reduces memory requirements:</p> <ul> <li>Constant Memory Usage: Memory consumption doesn't grow with dataset size</li> <li>Resource Optimization: Efficient use of system resources</li> <li>Large Dataset Support: Can handle file systems with millions of files</li> </ul>"},{"location":"streaming/#performance","title":"Performance","text":"<p>Streaming improves performance in several ways:</p> <ul> <li>Reduced Latency: Start processing immediately without waiting for complete data</li> <li>Parallel Processing: Process multiple chunks concurrently</li> <li>I/O Optimization: Overlap I/O operations with processing</li> </ul>"},{"location":"streaming/#user-experience","title":"User Experience","text":"<p>Streaming enhances the user experience:</p> <ul> <li>Progress Reporting: Provide real-time feedback on long-running operations</li> <li>Responsiveness: Application remains responsive during processing</li> <li>Cancellation: Operations can be cancelled mid-stream</li> </ul>"},{"location":"streaming/#implementation-details","title":"Implementation Details","text":""},{"location":"streaming/#chunk-based-processing","title":"Chunk-Based Processing","text":"<p>FlashFS uses a chunk-based approach for streaming:</p> <pre><code>// Process data in chunks\nerr := processor.ProcessStream(reader, func(chunk Chunk) error {\n    // Process this chunk\n    processChunk(chunk.Data)\n\n    // Report progress\n    progress := float64(chunk.Index) / float64(chunk.Total) * 100\n    fmt.Printf(\"Progress: %.2f%%\\n\", progress)\n\n    return nil\n})\n</code></pre>"},{"location":"streaming/#buffered-io","title":"Buffered I/O","text":"<p>Streaming operations use buffered I/O for performance:</p> <pre><code>// Create a buffered reader/writer\nbufReader := bufio.NewReaderSize(reader, bufferSize)\nbufWriter := bufio.NewWriterSize(writer, bufferSize)\n</code></pre>"},{"location":"streaming/#parallel-chunk-processing","title":"Parallel Chunk Processing","text":"<p>Chunks can be processed in parallel for improved performance:</p> <pre><code>// Create a worker pool\nvar wg sync.WaitGroup\nchunkChan := make(chan Chunk, 10)\n\n// Start worker goroutines\nfor i := 0; i &lt; runtime.NumCPU(); i++ {\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        for chunk := range chunkChan {\n            processChunk(chunk)\n        }\n    }()\n}\n\n// Feed chunks to workers\nprocessor.ProcessStream(reader, func(chunk Chunk) error {\n    chunkChan &lt;- chunk\n    return nil\n})\nclose(chunkChan)\n\n// Wait for all workers to finish\nwg.Wait()\n</code></pre>"},{"location":"streaming/#configuration","title":"Configuration","text":"<p>Streaming behavior can be configured through options:</p> <pre><code>options := StreamingOptions{\n    ChunkSize:  5000,     // Number of entries per chunk\n    BufferSize: 128*1024, // Buffer size for I/O operations\n    NumWorkers: 4,        // Number of worker goroutines\n}\n</code></pre>"},{"location":"streaming/#use-cases","title":"Use Cases","text":""},{"location":"streaming/#very-large-file-systems","title":"Very Large File Systems","text":"<p>Streaming is essential for processing very large file systems:</p> <ul> <li>Enterprise Storage: File systems with millions of files</li> <li>Media Archives: Large collections of media files</li> <li>Source Code Repositories: Large monorepos with many files</li> </ul>"},{"location":"streaming/#network-transfers","title":"Network Transfers","text":"<p>Streaming enables efficient network transfers:</p> <ul> <li>Cloud Backups: Stream data directly to cloud storage</li> <li>Remote Synchronization: Efficiently sync changes between systems</li> <li>Distributed Processing: Process data across multiple nodes</li> </ul>"},{"location":"streaming/#resource-constrained-environments","title":"Resource-Constrained Environments","text":"<p>Streaming is valuable in resource-constrained environments:</p> <ul> <li>Embedded Systems: Devices with limited memory</li> <li>Shared Servers: Systems where memory must be conserved</li> <li>Container Environments: Containerized applications with memory limits</li> </ul>"},{"location":"walker/","title":"Walker","text":"<p>The Walker component in FlashFS is responsible for traversing file systems and collecting metadata about files and directories. It provides the foundation for creating snapshots by efficiently gathering information about the file system structure.</p>"},{"location":"walker/#overview","title":"Overview","text":"<p>The Walker component:</p> <ul> <li>Traverses directories recursively</li> <li>Collects metadata about files and directories</li> <li>Computes content hashes for files</li> <li>Handles errors and edge cases during traversal</li> <li>Supports context-based cancellation</li> <li>Provides streaming implementations for memory efficiency and responsiveness</li> </ul>"},{"location":"walker/#walker-implementations","title":"Walker Implementations","text":"<p>FlashFS provides multiple implementations for walking directory trees, each with different characteristics and use cases:</p>"},{"location":"walker/#standard-walker-walk","title":"Standard Walker (<code>Walk</code>)","text":"<p>The standard walker is a non-streaming implementation that collects all entries in memory before returning them as a slice.</p> <pre><code>entries, err := walker.Walk(rootDir)\nif err != nil {\n    // handle error\n}\n\nfor _, entry := range entries {\n    // process entry\n}\n</code></pre>"},{"location":"walker/#callback-based-streaming-walker-walkstreamwithcallback","title":"Callback-based Streaming Walker (<code>WalkStreamWithCallback</code>)","text":"<p>The callback-based streaming walker processes entries as they're discovered, calling a user-provided function for each entry.</p> <pre><code>err := walker.WalkStreamWithCallback(context.Background(), rootDir, walker.DefaultWalkOptions(), \n    func(entry walker.SnapshotEntry) error {\n        // process entry\n        return nil // return an error to stop walking\n    })\nif err != nil {\n    // handle error\n}\n</code></pre>"},{"location":"walker/#channel-based-streaming-walker-walkstream","title":"Channel-based Streaming Walker (<code>WalkStream</code>)","text":"<p>The channel-based streaming walker returns channels for entries and errors, allowing for concurrent processing.</p> <pre><code>entryChan, errChan := walker.WalkStream(context.Background(), rootDir)\n\n// Process entries as they arrive\nfor entry := range entryChan {\n    // process entry\n}\n\n// Check for errors after all entries have been processed\nif err := &lt;-errChan; err != nil {\n    // handle error\n}\n</code></pre>"},{"location":"walker/#core-data-structure","title":"Core Data Structure","text":"<p>The primary data structure used by the Walker is the <code>SnapshotEntry</code>:</p> <pre><code>type SnapshotEntry struct {\n    Path          string      // Relative path from the root\n    Size          int64       // Size in bytes\n    ModTime       time.Time   // Modification time\n    IsDir         bool        // True if it's a directory\n    Permissions   fs.FileMode // File permissions\n    Hash          string      // Hash of the file content (if computed)\n    SymlinkTarget string      // Target of the symlink (if it's a symlink)\n    Error         error\n}\n</code></pre> <p>This structure captures essential information about each file or directory.</p>"},{"location":"walker/#performance-characteristics","title":"Performance Characteristics","text":"<p>Based on benchmarks, here are the performance characteristics of each implementation:</p>"},{"location":"walker/#with-hashing-enabled","title":"With Hashing Enabled","text":"Implementation Operations/sec Time/op Memory/op Allocations/op StandardWalkDir (Go stdlib) 4,286 261.9 \u00b5s 63.2 KB 757 Walk 631 1.86 ms 12.3 MB 4,678 WalkStreamWithCallback 579 2.09 ms 13.0 MB 4,437 WalkStream 579 2.11 ms 13.7 MB 4,441"},{"location":"walker/#without-hashing","title":"Without Hashing","text":"Implementation Operations/sec Time/op Memory/op Allocations/op Walk 1,642 728.7 \u00b5s 277.1 KB 2,077 WalkStreamWithCallback 1,101 1.09 ms 185.4 KB 1,636 WalkStream 1,056 1.11 ms 188.1 KB 1,641"},{"location":"walker/#when-to-use-each-implementation","title":"When to Use Each Implementation","text":""},{"location":"walker/#use-the-standard-walker-walk-when","title":"Use the Standard Walker (<code>Walk</code>) when","text":"<ul> <li>You need all entries before processing can begin</li> <li>The directory structure is small to medium-sized</li> <li>Simplicity is preferred over advanced features</li> <li>You want slightly better performance for small to medium-sized directories</li> </ul>"},{"location":"walker/#use-the-callback-based-streaming-walker-walkstreamwithcallback-when","title":"Use the Callback-based Streaming Walker (<code>WalkStreamWithCallback</code>) when","text":"<ul> <li>You want to process entries as they're discovered</li> <li>You prefer a callback-based programming style</li> <li>You need to handle very large directory structures</li> <li>You want to provide progress updates during the walk</li> <li>Memory efficiency is important</li> </ul>"},{"location":"walker/#use-the-channel-based-streaming-walker-walkstream-when","title":"Use the Channel-based Streaming Walker (<code>WalkStream</code>) when","text":"<ul> <li>You want to process entries as they're discovered</li> <li>You prefer a channel-based programming style</li> <li>You need to integrate with other Go concurrency patterns</li> <li>You want to process entries concurrently with other operations</li> <li>You need to handle very large directory structures</li> <li>Memory efficiency is important</li> </ul>"},{"location":"walker/#scalability-considerations","title":"Scalability Considerations","text":"<p>The streaming walker implementations (both callback and channel-based) offer significant advantages for large directory structures:</p> <ol> <li> <p>Memory Efficiency: Since entries are processed as they're discovered, the memory footprint remains relatively constant regardless of the directory size.</p> </li> <li> <p>Responsiveness: Users can start processing entries immediately, rather than waiting for the entire walk to complete.</p> </li> <li> <p>Cancellation: Operations can be cancelled mid-walk using context cancellation.</p> </li> <li> <p>Progress Reporting: Real-time progress updates can be provided during the walk.</p> </li> </ol> <p>For very large directory structures (millions of files), the streaming implementations are strongly recommended to avoid out-of-memory errors and provide better user experience.</p>"},{"location":"walker/#configuration-options","title":"Configuration Options","text":"<p>The Walker component can be configured using the <code>WalkOptions</code> struct:</p> <pre><code>type WalkOptions struct {\n    // ComputeHashes determines whether file hashes should be computed.\n    ComputeHashes bool\n    // FollowSymlinks determines whether symbolic links should be followed.\n    FollowSymlinks bool\n    // MaxDepth is the maximum directory depth to traverse (0 means no limit).\n    MaxDepth int\n    // NumWorkers specifies the number of worker goroutines for parallel processing.\n    NumWorkers int\n    // HashAlgorithm specifies the algorithm to be used like \"BLAKE3\", \"MD5\" etc\n    HashAlgorithm string\n    // SkipErrors specifies that errors should not cancel operations\n    SkipErrors bool\n    // ExcludePatterns is a list of filepath.Match patterns to exclude from the walk.\n    ExcludePatterns []string\n    // UsePartialHashing enables partial hashing for large files.\n    UsePartialHashing bool\n    // PartialHashingThreshold is the file size threshold in bytes above which\n    // partial hashing will be used (if enabled). Default is 10MB.\n    PartialHashingThreshold int64\n}\n</code></pre> <p>Default options can be obtained using:</p> <pre><code>options := walker.DefaultWalkOptions()\n</code></pre>"},{"location":"walker/#example-processing-a-large-directory-tree","title":"Example: Processing a Large Directory Tree","text":"<pre><code>// Using the callback-based streaming walker\nprocessedCount := 0\ntotalCount := 0\n\nerr := walker.WalkStreamWithCallback(ctx, rootDir, walker.DefaultWalkOptions(), \n    func(entry walker.SnapshotEntry) error {\n        processedCount++\n\n        // Process the entry\n        if !entry.IsDir {\n            // Do something with the file\n            fmt.Printf(\"Processing file %s (%d/%d)\\n\", entry.Path, processedCount, totalCount)\n        }\n\n        return nil\n    })\n\nif err != nil {\n    fmt.Printf(\"Error walking directory: %v\\n\", err)\n}\n</code></pre>"},{"location":"walker/#example-concurrent-processing-with-channels","title":"Example: Concurrent Processing with Channels","text":"<pre><code>// Using the channel-based streaming walker\nentryChan, errChan := walker.WalkStream(ctx, rootDir)\n\n// Create a worker pool\nconst numWorkers = 4\nvar wg sync.WaitGroup\nwg.Add(numWorkers)\n\n// Start workers\nfor i := 0; i &lt; numWorkers; i++ {\n    go func() {\n        defer wg.Done()\n        for entry := range entryChan {\n            if !entry.IsDir {\n                // Process the file\n                processFile(entry)\n            }\n        }\n    }()\n}\n\n// Wait for all entries to be processed\nwg.Wait()\n\n// Check for errors\nif err := &lt;-errChan; err != nil {\n    fmt.Printf(\"Error walking directory: %v\\n\", err)\n}\n</code></pre>"},{"location":"walker/#integration-with-other-components","title":"Integration with Other Components","text":"<p>The Walker integrates with:</p> <ul> <li>Serializer: Provides file metadata to be serialized</li> <li>Storage: Indirectly supplies the data for snapshots</li> <li>Diff: Enables comparison by providing consistent metadata</li> </ul>"},{"location":"walker/#conclusion","title":"Conclusion","text":"<p>FlashFS provides multiple walker implementations to suit different use cases and programming styles. For most applications, the streaming implementations offer the best balance of features, scalability, and usability, especially for large directory structures. The standard walker provides slightly better performance for small to medium-sized directories when all entries need to be collected before processing.</p>"}]}